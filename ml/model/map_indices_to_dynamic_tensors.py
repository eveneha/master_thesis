import json
import torch # Need torch for tensor operations in remapping
from functools import reduce

# --- Load Original Traceback Map ---
try:
    # Use the corrected JSON file generated by the refined indices.py
    with open("relevant_indices_per_layer.json", "r") as f:
        original_indices_map = json.load(f)
    print("Loaded original_indices_per_layer.json")

    # Convert lists to sorted LongTensors immediately for processing
    tensor_original_map = {}
    for key, value in original_indices_map.items():
        if isinstance(value, list):
            tensor_original_map[key] = torch.tensor(sorted(list(set(value))), dtype=torch.long)
        else:
            print(f"Warning: Value for key '{key}' in original map is not a list. Skipping.")
            tensor_original_map[key] = None
    original_indices_map = tensor_original_map # Use the tensor map

except FileNotFoundError:
    print("ERROR: relevant_indices_per_layer.json not found. Please run the corrected indices.py first.")
    exit()
except json.JSONDecodeError:
     print("ERROR: relevant_indices_per_layer.json is not valid JSON.")
     exit()

# --- Helper Function for Remapping (from previous response) ---
def remap_indices(original_relevant_indices_tensor, sorted_kept_indices_tensor):
    """Remaps original indices to their new positions within a sliced tensor."""
    if original_relevant_indices_tensor is None:
        return None
    if sorted_kept_indices_tensor is None or sorted_kept_indices_tensor.numel() == 0:
        # If nothing was kept previously, we can't select anything now.
        return torch.tensor([], dtype=torch.long) # Return empty tensor

    # Find which original relevant indices are present in the kept indices
    device = sorted_kept_indices_tensor.device # Use device of kept_indices
    is_present = (original_relevant_indices_tensor.to(device).unsqueeze(1) == sorted_kept_indices_tensor.unsqueeze(0)).any(dim=1)
    present_original_indices = original_relevant_indices_tensor[is_present]

    if present_original_indices.numel() == 0:
        return torch.tensor([], dtype=torch.long)

    # Find the positions (new indices) of the present indices within the sorted kept list
    new_remapped_indices = torch.searchsorted(sorted_kept_indices_tensor, present_original_indices.to(device))
    return new_remapped_indices.to(torch.long) # Ensure output is Long

# --- Function to Simulate Slicing and Create Remapped Map ---
def create_remapped_node_index_map(original_map):
    """
    Simulates slicing and creates a map with remapped indices for each node.

    Returns:
        dict: Map from node name to remapped (relative) indices (as lists for JSON).
    """
    remapped_node_map = {}
    device = torch.device('cpu') # Perform remapping on CPU

    # --- Stage 0: Input ---
    # Indices relevant FROM the original input tensor (needed FOR block1.conv1)
    indices_needed_from_input = original_map.get("block1_conv1")
    if indices_needed_from_input is not None:
         indices_needed_from_input = indices_needed_from_input.to(device)
         # No remapping needed, these are the first set of absolute indices
         remapped_node_map["global_in"] = indices_needed_from_input.tolist()
         kept_indices_absolute = indices_needed_from_input # Keep track
    else:
         remapped_node_map["global_in"] = [] # Or indicate 'all'/'none'? Empty list safer.
         kept_indices_absolute = None # Need a way to represent "all kept" if no slice
         print("Warning: No indices found for input ('block1_conv1'). Assuming all kept initially (this might require adjustments).")
         # If no input slice, 'kept_indices_absolute' should represent all original input indices
         # This requires knowing original input size, which isn't directly in the map.
         # For simplicity, let's assume if the first key is missing, NO slicing happens anywhere.
         # A better approach might be to pass original_input_len here.
         # Workaround: If indices_needed_from_input is None, assume no slicing happens AT ALL.
         if indices_needed_from_input is None:
              print("Assuming no slicing will occur anywhere due to missing input indices.")
              # Create dummy entries for other keys based on original map?
              # Or just return an empty map? Returning potentially incorrect map is bad.
              # Let's proceed assuming the key EXISTS, otherwise logic fails.
              if kept_indices_absolute is None:
                   print("ERROR: Cannot proceed without knowing initial kept indices ('block1_conv1' missing in source map?)")
                   return {} # Return empty dict on error


    # --- Stage 1: After block 0, conv 1 output ---
    # Indices relevant FROM block1.conv1 output (needed FOR block1.conv2)
    original_indices_stage1 = original_map.get("block1_conv2")
    if original_indices_stage1 is not None:
        original_indices_stage1 = original_indices_stage1.to(device)
        # Remap based on indices kept *before* this stage (kept_indices_absolute)
        remapped_indices_stage1 = remap_indices(original_indices_stage1, kept_indices_absolute)
        remapped_node_map["temporal_blocks.0.conv1"] = remapped_indices_stage1.tolist()
        # Update kept_indices_absolute based on the remapped selection
        if remapped_indices_stage1.numel() > 0:
             # Bounds check before indexing
             if kept_indices_absolute is not None and torch.max(remapped_indices_stage1) < kept_indices_absolute.numel():
                  kept_indices_absolute = kept_indices_absolute[remapped_indices_stage1]
             else:
                  print(f"Error/Warning: Remapped indices for stage 1 out of bounds for kept_indices_absolute ({kept_indices_absolute.numel() if kept_indices_absolute is not None else 'None'}). Setting kept to empty.")
                  kept_indices_absolute = torch.tensor([], dtype=torch.long, device=device)
        else: # No relevant indices remain after remapping/slicing
             kept_indices_absolute = torch.tensor([], dtype=torch.long, device=device)
    else: # No indices specified for this stage in the original map
        remapped_node_map["temporal_blocks.0.conv1"] = [] # Indicate no slicing *at this stage*
        # kept_indices_absolute remains unchanged if no slicing happens here

    # --- Stage 2: After block 0, conv 2 output ---
    # Indices relevant FROM block1.conv2 output (needed FOR block2.conv1)
    original_indices_stage2 = original_map.get("block2_conv1")
    if original_indices_stage2 is not None:
        original_indices_stage2 = original_indices_stage2.to(device)
        remapped_indices_stage2 = remap_indices(original_indices_stage2, kept_indices_absolute)
        remapped_node_map["temporal_blocks.0.conv2"] = remapped_indices_stage2.tolist()
        if remapped_indices_stage2.numel() > 0:
             if kept_indices_absolute is not None and torch.max(remapped_indices_stage2) < kept_indices_absolute.numel():
                  kept_indices_absolute = kept_indices_absolute[remapped_indices_stage2]
             else:
                  print(f"Error/Warning: Remapped indices for stage 2 out of bounds for kept_indices_absolute ({kept_indices_absolute.numel() if kept_indices_absolute is not None else 'None'}). Setting kept to empty.")
                  kept_indices_absolute = torch.tensor([], dtype=torch.long, device=device)
        else:
             kept_indices_absolute = torch.tensor([], dtype=torch.long, device=device)
    else:
        remapped_node_map["temporal_blocks.0.conv2"] = []


    # --- Stage 3: After block 1, conv 1 output ---
    original_indices_stage3 = original_map.get("block2_conv2")
    if original_indices_stage3 is not None:
        original_indices_stage3 = original_indices_stage3.to(device)
        remapped_indices_stage3 = remap_indices(original_indices_stage3, kept_indices_absolute)
        remapped_node_map["temporal_blocks.1.conv1"] = remapped_indices_stage3.tolist()
        if remapped_indices_stage3.numel() > 0:
             if kept_indices_absolute is not None and torch.max(remapped_indices_stage3) < kept_indices_absolute.numel():
                  kept_indices_absolute = kept_indices_absolute[remapped_indices_stage3]
             else:
                  print(f"Error/Warning: Remapped indices for stage 3 out of bounds for kept_indices_absolute ({kept_indices_absolute.numel() if kept_indices_absolute is not None else 'None'}). Setting kept to empty.")
                  kept_indices_absolute = torch.tensor([], dtype=torch.long, device=device)
        else:
             kept_indices_absolute = torch.tensor([], dtype=torch.long, device=device)
    else:
        remapped_node_map["temporal_blocks.1.conv1"] = []


    # --- Stage 4: After block 1, conv 2 output ---
    # *** Using corrected key "block3_conv1" ***
    original_indices_stage4 = original_map.get("block3_conv1")
    if original_indices_stage4 is not None:
        original_indices_stage4 = original_indices_stage4.to(device)
        remapped_indices_stage4 = remap_indices(original_indices_stage4, kept_indices_absolute)
        remapped_node_map["temporal_blocks.1.conv2"] = remapped_indices_stage4.tolist()
        if remapped_indices_stage4.numel() > 0:
             if kept_indices_absolute is not None and torch.max(remapped_indices_stage4) < kept_indices_absolute.numel():
                   kept_indices_absolute = kept_indices_absolute[remapped_indices_stage4]
             else:
                  print(f"Error/Warning: Remapped indices for stage 4 out of bounds for kept_indices_absolute ({kept_indices_absolute.numel() if kept_indices_absolute is not None else 'None'}). Setting kept to empty.")
                  kept_indices_absolute = torch.tensor([], dtype=torch.long, device=device)
        else:
             kept_indices_absolute = torch.tensor([], dtype=torch.long, device=device)
    else:
        remapped_node_map["temporal_blocks.1.conv2"] = []


    # --- Stage 5: After block 2, conv 1 output ---
    original_indices_stage5 = original_map.get("block3_conv2")
    if original_indices_stage5 is not None:
        original_indices_stage5 = original_indices_stage5.to(device)
        remapped_indices_stage5 = remap_indices(original_indices_stage5, kept_indices_absolute)
        remapped_node_map["temporal_blocks.2.conv1"] = remapped_indices_stage5.tolist()
        if remapped_indices_stage5.numel() > 0:
             if kept_indices_absolute is not None and torch.max(remapped_indices_stage5) < kept_indices_absolute.numel():
                   kept_indices_absolute = kept_indices_absolute[remapped_indices_stage5]
             else:
                  print(f"Error/Warning: Remapped indices for stage 5 out of bounds for kept_indices_absolute ({kept_indices_absolute.numel() if kept_indices_absolute is not None else 'None'}). Setting kept to empty.")
                  kept_indices_absolute = torch.tensor([], dtype=torch.long, device=device)
        else:
             kept_indices_absolute = torch.tensor([], dtype=torch.long, device=device)
    else:
        remapped_node_map["temporal_blocks.2.conv1"] = []


    # --- Stage 6: After block 2, conv 2 output (Final Selection) ---
    original_indices_stage6 = original_map.get("block_output")
    if original_indices_stage6 is not None:
        original_indices_stage6 = original_indices_stage6.to(device)
        remapped_indices_stage6 = remap_indices(original_indices_stage6, kept_indices_absolute)
        remapped_node_map["temporal_blocks.2.conv2"] = remapped_indices_stage6.tolist()
        # We don't need to update kept_indices_absolute after the final stage
    else:
        remapped_node_map["temporal_blocks.2.conv2"] = []

    return remapped_node_map

# --- Helper to print the map ---
def print_remapped_indices_map(indices_map):
    """Prints a summary of the remapped indices map."""
    print("\nüìã Remapped Indices Per Node (For Direct Slicing):\n")
    max_key_len = max(len(k) for k in indices_map.keys()) if indices_map else 0
    for node, indices in indices_map.items():
        key_str = f"üîπ {node}:".ljust(max_key_len + 4)
        if not isinstance(indices, list):
            print(f"{key_str}‚ùì Invalid format (expected list)")
            continue
        count = len(indices)
        if count == 0:
             print(f"{key_str} ‚ùå 0 indices (Empty list - slice result will be empty)")
             continue

        first = indices[:5]
        last = indices[-5:] if count > 5 else []
        preview = f"{first} ... {last}" if last else f"{first}"
        print(f"{key_str} {count} remapped indices ‚Üí {preview}")


# --- Execution ---
print("Creating remapped index map by simulating slicing...")
remapped_indices_per_node = create_remapped_node_index_map(original_indices_map)

# Print the created map for verification
print_remapped_indices_map(remapped_indices_per_node)

# Save the relevant indices per node to the target JSON file
output_filename = "remapped_relevant_indices_per_model_layer.json" # New filename
try:
    # Ensure keys exist even if list is empty before saving
    expected_keys = [
        "global_in", "temporal_blocks.0.conv1", "temporal_blocks.0.conv2",
        "temporal_blocks.1.conv1", "temporal_blocks.1.conv2",
        "temporal_blocks.2.conv1", "temporal_blocks.2.conv2"
    ]
    output_map_final = {key: remapped_indices_per_node.get(key, []) for key in expected_keys}

    with open(output_filename, "w") as f:
        json.dump(output_map_final, f, indent=2)
    print(f"\n‚úÖ Successfully saved **remapped** node-to-indices map to '{output_filename}'")
except Exception as e:
    print(f"\n‚ùå Error saving JSON file '{output_filename}': {e}")